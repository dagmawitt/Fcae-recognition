# -*- coding: utf-8 -*-
"""LSVM with Acc, Spec and sen.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nOkm40HzQQalFWNA8Ag57Bnx_HcU8RL_
"""

!pip install tensorflow_addons

# ==================== important packages =====================#
# =============================================================#
import cv2
import os
import keras
from keras import Model
from keras import layers
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import tensorflow_addons as tfa
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib as plt
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.decomposition import PCA
from sklearn.svm import LinearSVC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, accuracy_score
from time import time
import csv
from sklearn.datasets import fetch_lfw_people
from sklearn.utils.fixes import loguniform
from imblearn.metrics import sensitivity_score
from imblearn.metrics import specificity_score


#######################  dataset ####################


lfw_people = fetch_lfw_people(min_faces_per_person=80, resize=0.5)

n_samples, h, w = lfw_people.images.shape

X = lfw_people.data

n_features = X.shape[1]

df = pd.DataFrame(X)

df.to_csv(r'C:\Users\DB\Desktop\export_dataframe.csv', index=False, header=True)

y = lfw_people.target

n_components = 150

target_names = lfw_people.target_names

n_classes = target_names.shape[0]

learning_rate = 0.001

weight_decay = 0.001

batch_size = 128

num_epochs = 10

print("Total dataset size:")
print("n_samples: %d" % n_samples)
print("n_features: %d" % n_features)
print("n_classes: %d" % n_classes)
print('Learning rate:',learning_rate)
print('Weight:',weight_decay)
print('batch_size:',batch_size)
print('Number of epochs:',num_epochs)

print('===== start training here==============')


X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

print('================ Convmixer model ================')

def activation_block(x):
    x = layers.Activation("gelu")(x)
    return layers.BatchNormalization()(x)

def conv_stem(x, filters: int, patch_size: int):
    x = layers.Conv2D(filters, kernel_size=patch_size, strides=patch_size)(x)
    return activation_block(x)

def conv_mixer_block(x, filters: int, kernel_size: int):
    x0 = x
    x = layers.DepthwiseConv2D(kernel_size=kernel_size, padding="same")(x)
    x = layers.Add()([activation_block(x), x0])
    x = layers.Conv2D(filters, kernel_size=1)(x)
    x = activation_block(x)
    return x

def get_conv_mixer_256_8(
        image_size=32, filters=256, depth=8, kernel_size=5, patch_size=2, num_classes=10
):
    inputs = keras.Input((image_size, image_size, 3))
    x = layers.Rescaling(scale=1.0 / 255)(inputs)
    x = conv_stem(x, filters, patch_size)
    for _ in range(depth):
        x = conv_mixer_block(x, filters, kernel_size)

    x = layers.GlobalAvgPool2D()(x)
    outputs = layers.Dense(num_classes, activation="softmax")(x)

    return Model(inputs, outputs)


#=================== Convmixer End ==================================#
conv_mixer_model = get_conv_mixer_256_8()

print('==================== Part III starting =============================')


val_split = 0.1

val_indices = int(len(X_train) * val_split)

new_x_train, new_y_train = X_train[val_indices:], y_train[val_indices:]
x_val, y_val = X_train[:val_indices], y_train[:val_indices]


print(f"Training data samples: {len(new_x_train)}")

print(f"Validation data samples: {len(x_val)}")

image_size = 32

auto = tf.data.AUTOTUNE

data_augmentation = keras.Sequential(
    [layers.RandomCrop(image_size, image_size), layers.RandomFlip("horizontal"), ],
    name="data_augmentation",
)

#####################################################################################


def run_experiment(model):
    optimizer = tfa.optimizers.AdamW(
        learning_rate=learning_rate, weight_decay=weight_decay
    )

    model.compile(
        optimizer=optimizer,
        loss="sparse_categorical_crossentropy",
        metrics=['accuracy']
    )

    checkpoint_filepath = "/tmp/checkpoint"
    checkpoint_callback = keras.callbacks.ModelCheckpoint(
        checkpoint_filepath,
        monitor="val_accuracy",
        save_best_only=True,
    )

    history = model.fit(
       epochs=num_epochs,
       callbacks=[checkpoint_callback],
    )

    return history, model


#####################################################




t0 = time()

pca = PCA(n_components=n_components, whiten=True).fit(X_train)


print("done in %0.3fs" % (time() - t0))


X_train_pca = pca.transform(X_train)

df_1 = pd.DataFrame(X_train_pca)

df_1.to_csv(r'C:\Users\DB\Desktop\train.csv', index=False, header=True)


X_test_pca = pca.transform(X_test)

print("done in %0.3fs" % (time() - t0))

t0 = time()

param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],
              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }

print('================================================================')

lsvc = LinearSVC(verbose=0)

clf = make_pipeline(StandardScaler(),
                    LinearSVC(random_state=0, tol=1e-5))

clf = clf.fit(X_train_pca, y_train)

Classifier = SVC(kernel="linear")

Classifier.fit(X_train, y_train)

y_pred = Classifier.predict(X_test)

accuracy = accuracy_score(y_test,y_pred)*100

confusion_mat = confusion_matrix(y_test,y_pred)

print("Accuracy for LSVM is:",accuracy)
new = specificity_score(y_test, y_pred, average='macro')
print('specificity of lsvm',new)
new_1 = sensitivity_score(y_test, y_pred, average='macro')
print('sensitivity lsvm :',new_1)

print("Confusion Matrix")

print(confusion_mat)

print('================================================================')

print(' For Poly Kernel Accuracy:')

svclassifier = SVC(kernel='poly')

svclassifier.fit(X_train, y_train)

y_pred = svclassifier.predict(X_test)

accuracy = accuracy_score(y_test,y_pred)*100

confusion_mat = confusion_matrix(y_test,y_pred)
new = specificity_score(y_test, y_pred, average='macro')
print('specificity for poly is',new)
new_1 = sensitivity_score(y_test, y_pred, average='macro')
print('sensitivity for poly is :',new_1)

print("Accuracy for Poly is:",accuracy)

print("Confusion Matrix")

print(confusion_mat)

print('================================================================')

print(' For Gaussian Kernel Accuracy:')

gausssvclassifier = SVC(kernel='rbf')

gausssvclassifier.fit(X_train, y_train)

y_pred = gausssvclassifier.predict(X_test)

accuracy = accuracy_score(y_test,y_pred)*100

confusion_mat = confusion_matrix(y_test,y_pred)
new = specificity_score(y_test, y_pred, average='macro')
print('specificity Gaussian is',new)

new_1 = sensitivity_score(y_test, y_pred, average='macro')
print('sensitivity Gaussian is :',new_1)

print("Accuracy for Gaussian is:",accuracy)

print("Confusion Matrix")

print(confusion_mat)



##################################################################

print("Predicting people's names on the test set")
t0 = time()
y_pred = clf.predict(X_test_pca)
print("done in %0.3fs" % (time() - t0))

print(classification_report(y_test, y_pred, target_names=target_names))
ConfusionMatrixDisplay.from_estimator(
    clf, X_test_pca, y_test, display_labels=target_names, xticks_rotation="vertical"
)

####################################################################


#"""


print('===========================================================')


print('Camera opening...')

faceCascade = cv2.CascadeClassifier('data1/haarcascade_frontalface_default.xml')

eyeCascade = cv2.CascadeClassifier('data1/haarcascade_eye_tree_eyeglasses.xml')

camera = cv2.VideoCapture(0)

camera.set(3,640)

camera.set(4,480)

while (True):
    test = []
    face = []

    return_value, image = camera.read()
    xv, yv, cv = image.shape
    image = cv2.flip(image, 1)
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    cv2.imshow("image", image)
    faces = faceCascade.detectMultiScale(
        gray,
        scaleFactor=1.3,
        minNeighbors=5,
        minSize=(30, 30)
    )
    if len(faces) == 0:
        print('No faces found')
    else:
      print('face detected')

    for (x, y, wf, hf) in faces:
        cy, cx = y + (hf // 2), x + (wf // 2)
        max_len = max(max(hf // 2, wf // 2), 125)
        roi_gray = gray[y:y + hf, x:x + wf]
        roi_color = image[y:y + hf, x:x + wf]

        eyes = eyeCascade.detectMultiScale(
            roi_gray,
            scaleFactor=1.5,
            minNeighbors=10,
            minSize=(5, 5),
        )

        for (ex, ey, ew, eh) in eyes:
            cv2.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)

        if (x - max_len) <= 0 or (x + max_len) >= xv or (y - max_len) <= 0 or (y + max_len) >= yv:
            continue
        face_crop = (image[cy - max_len:cy + max_len, cx - max_len:cx + max_len])
      #  face_crop = face_crop[data_slice[0]:data_slice[1], data_slice[2]:data_slice[3]]
        testImage = cv2.resize(face_crop, (w, h))
        cv2.imshow('face', testImage)
        testImage = cv2.cvtColor(testImage, cv2.COLOR_BGR2GRAY)
        testImageFeatureVector = np.array(testImage).flatten()
        test.append(testImageFeatureVector)
        testImagePCA = pca.transform(test)
        testImagePredict = clf.predict(testImagePCA)
        image = cv2.rectangle(image, (x, y), (x + wf, y + hf), (255, 0, 0), 1)
        image = cv2.rectangle(image, (x, y + hf), (x + wf, y + hf + 30), (255, 0, 0), -1)
        cv2.putText(image, "Name : " + target_names[testImagePredict[0]], (x + x // 10, y + hf + 20), \
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        cv2.imshow('image', image)


    k = cv2.waitKey(30) & 0xff
    if k == 27:
      break
del(camera)

#"""

import matplotlib.pyplot as plt
cutoff_df = pd.DataFrame( columns = ['Accuracy','Sensitivity','Specificity'])
plt.show()